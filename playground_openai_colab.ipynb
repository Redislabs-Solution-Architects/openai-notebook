{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3ead2f",
   "metadata": {},
   "source": [
    "# Playground for Chatbot Example App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae44bd0-cb66-4172-bb5b-a3f41265b776",
   "metadata": {},
   "source": [
    "<a target="_blank" href="https://colab.research.google.com/github/Redislabs-Solution-Architects/openai-notebook/blob/main/playground_openai_colab.ipynb">
    " <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b63cf",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2045e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio openai langchain langchain-openai redisvl pypdf sentence-transformers==2.2.2\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "import gradio as gr\n",
    "import tiktoken #required for OpenAI\n",
    "\n",
    "import redis as redisclient\n",
    "from redis.commands.search.field import VectorField\n",
    "\n",
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "from redisvl.utils.vectorize import OpenAITextVectorizer\n",
    "from redisvl.index import SearchIndex\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.vectorstores import Redis\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ee8e8",
   "metadata": {},
   "source": [
    "## Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66924c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Redislabs-Solution-Architects/openai-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0643a94",
   "metadata": {},
   "source": [
    "## Apply Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa10722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "DOCS_FOLDER = os.getenv(\"DOCS_FOLDER\", \"openai-notebook/pdfs/\")\n",
    "REDIS_URL = os.getenv(\"LOCAL_REDIS_URL\", \"redis://localhost:6379\")\n",
    "OPENAI_API_KEY = userdata.get(\"openai_api_key\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-ada-002\")\n",
    "OPENAI_CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4\")\n",
    "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", 500))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"CHUNK_OVERLAP\", 100))\n",
    "PAGE_TITLE = os.getenv(\"PAGE_TITLE\", \"ðŸ“ƒ Chat Your PDF\")\n",
    "PAGE_ICON = os.getenv(\"PAGE_ICON\", \"ðŸ“ƒ\")\n",
    "RETRIEVE_TOP_K = int(os.getenv(\"RETRIEVE_TOP_K\", 5))\n",
    "LLMCACHE_THRESHOLD = float(os.getenv(\"LLMCACHE_THRESHOLD\", 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac9045",
   "metadata": {},
   "source": [
    "## Extract Text from Documents and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427aaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = DOCS_FOLDER\n",
    "docs = []\n",
    "for file in os.listdir(path):\n",
    "    print(file, flush=True)\n",
    "    loader = PyPDFLoader(os.path.join(path, file))\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "#Show extracted pages\n",
    "def pagenumber_change(n):\n",
    "    page = docs[int(n)]\n",
    "    return page.page_content,page.metadata\n",
    "\n",
    "with gr.Blocks() as demopage:\n",
    "    gr.Markdown(\"### Pages Content\")\n",
    "    page = gr.TextArea(label=\"Page Content\",max_lines=10,value=docs[0].page_content)\n",
    "    meta_data = gr.Textbox(label=\"MetaData\",value=docs[0].metadata)\n",
    "    page_number = gr.Slider(label=\"Page Number\",  minimum=0, maximum= len(docs)-1, value=0,step=1, scale=1)\n",
    "    page_number.release(pagenumber_change, inputs=[page_number], outputs=[page,meta_data])\n",
    "        \n",
    "demopage.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd71a8c",
   "metadata": {},
   "source": [
    "## Split Documents and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa89b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#Show splits with overlaps\n",
    "def docnumber_change(n):\n",
    "    doc = splits[int(n)]\n",
    "    return doc.page_content,doc.metadata\n",
    "\n",
    "with gr.Blocks() as demodoc:\n",
    "    gr.Markdown(\"### View Split Documents\")\n",
    "    doc_content = gr.TextArea(label=\"Document Content\",max_lines=10,value=splits[0].page_content)\n",
    "    doc_meta_data = gr.Textbox(label=\"MetaData\",value=splits[0].metadata)\n",
    "    doc_number = gr.Slider(label=\"Document Number\",  minimum=0, maximum= len(splits)-1, value=0,step=1, scale=1)\n",
    "    doc_number.release(docnumber_change, inputs=[doc_number], outputs=[doc_content,doc_meta_data])\n",
    "        \n",
    "demodoc.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffbb21d",
   "metadata": {},
   "source": [
    "## Create Embeddings and Store in VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "\n",
    "# Check if not already vectorized (currently at path level, not at path/file level)\n",
    "embeddingsDone = redisclient.Redis.from_url(REDIS_URL)\n",
    "embeddingsDoneForDoc = embeddingsDone.sismember(\"doc:chatbot:path\", path)\n",
    "if not embeddingsDoneForDoc:\n",
    "    vectordb = None\n",
    "    for splitN in chunker(splits, 16):\n",
    "        if vectordb == None:\n",
    "            vectordb = Redis.from_documents(\n",
    "                splitN, embeddings, redis_url=REDIS_URL, index_name=\"chatbot\"\n",
    "            )\n",
    "        else:\n",
    "            vectordb.add_documents(splitN)\n",
    "    embeddingsDone.sadd(\"doc:chatbot:path\", path)\n",
    "else:\n",
    "    print(\"Found existing embeddings in doc:chatbot:path for \"+ path, flush=True)\n",
    "    schema = {\n",
    "        'text': [\n",
    "            {'name': 'source', \n",
    "             'weight': 1, \n",
    "             'no_stem': False, \n",
    "             'withsuffixtrie': False, \n",
    "             'no_index': False, \n",
    "             'sortable': False}, \n",
    "            {'name': 'content', \n",
    "             'weight': 1, \n",
    "             'no_stem': False, \n",
    "             'withsuffixtrie': False, \n",
    "             'no_index': False, \n",
    "             'sortable': False}\n",
    "        ], \n",
    "        'numeric': [\n",
    "            {'name': 'page', \n",
    "             'no_index': False, \n",
    "             'sortable': False}\n",
    "        ], \n",
    "        'vector': [\n",
    "            {'name': 'content_vector', \n",
    "             'dims': 1536, 'algorithm': 'FLAT', \n",
    "             'datatype': 'FLOAT32', \n",
    "             'distance_metric': 'COSINE', \n",
    "             'initial_cap': 20000, \n",
    "             'block_size': 1000}\n",
    "        ]\n",
    "    }\n",
    "    vectordb = Redis.from_existing_index(\n",
    "            embeddings,\n",
    "            index_name=\"chatbot\",\n",
    "            redis_url=REDIS_URL,\n",
    "            schema = schema\n",
    "    )\n",
    "\n",
    "print(vectordb.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87725e51",
   "metadata": {},
   "source": [
    "## Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": RETRIEVE_TOP_K})\n",
    "tool = create_retriever_tool(retriever, \"search_bmw_i4\",\n",
    "       \"Searches and returns snippets from the BMW i4 brochure.\")\n",
    "tools = list()\n",
    "tools.append(tool)\n",
    "#Show the returned tool\n",
    "print(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e797",
   "metadata": {},
   "source": [
    "## Set up the Redis LLMCache Built with OpenAI Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bafe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcache_embeddings = OpenAITextVectorizer(\n",
    "    api_config={'api_key': OPENAI_API_KEY},\n",
    "    model=OPENAI_EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "schema = {\n",
    "    \"index\": {\n",
    "        \"name\": \"cache\",\n",
    "        \"prefix\": \"llmcache\",\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"prompt_vector\",\n",
    "            \"type\": \"vector\",\n",
    "            \"attrs\": {\n",
    "                \"dims\": 1536,\n",
    "                \"distance_metric\": \"cosine\",\n",
    "                \"algorithm\": \"flat\",\n",
    "                \"datatype\": \"float32\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "cache = SearchIndex.from_dict(schema)\n",
    "cache.connect(REDIS_URL)\n",
    "cache.create(overwrite=True)\n",
    "\n",
    "llmcache = SemanticCache(\n",
    "    redis_url=REDIS_URL,\n",
    "    threshold=LLMCACHE_THRESHOLD, # semantic similarity threshold\n",
    "    vectorizer=llmcache_embeddings,\n",
    "    index=cache\n",
    ")\n",
    "print(llmcache.index.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55befa25",
   "metadata": {},
   "source": [
    "## Configure the Conversational Chat Agent that Can Use the Redis Vector DB for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Redis memory for conversation history\n",
    "msgs = RedisChatMessageHistory(\n",
    "    session_id=\"my_session\", url=REDIS_URL\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", chat_memory=msgs, return_messages=True\n",
    ")\n",
    "chatLLM = ChatOpenAI(\n",
    "    model_name=OPENAI_CHAT_MODEL,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True\n",
    ")\n",
    "PREFIX = \"\"\"\"You are a friendly AI assistant that can provide information on the BMW i4 based on the provided PDF manual. Users can ask questions of your manual! Only use information from the prompt. You should not make anything up.\"\"\"\n",
    "\n",
    "FORMAT_INSTRUCTIONS = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "'''\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "'''\n",
    "\n",
    "When you have gathered all the information required, respond to the user in a friendly manner.\n",
    "\"\"\"\n",
    "\n",
    "SUFFIX = \"\"\"\n",
    "\n",
    "Begin! Remember to give detailed, informative answers\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    chatLLM,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    agent_kwargs={\n",
    "        'prefix': PREFIX,\n",
    "        'format_instructions': FORMAT_INSTRUCTIONS,\n",
    "        'suffix': SUFFIX\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da084a",
   "metadata": {},
   "source": [
    "## Generate a Response to the User's Question after Checking the Cache (if Enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b178263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "def generate_response(\n",
    "    use_cache: bool,\n",
    "    llmcache: SemanticCache,\n",
    "    user_query: str,\n",
    "    agent\n",
    ") -> str:\n",
    "    t0 = time()\n",
    "    if use_cache:\n",
    "        if response := llmcache.check(user_query):\n",
    "            print(\"Cache Response Time (secs)\", time()-t0, flush=True)\n",
    "            return response[0][\"response\"]\n",
    " \n",
    "    retrieval_handler = BaseCallbackHandler()\n",
    "    response = agent.invoke(input=user_query, callbacks=[retrieval_handler])[\"output\"]\n",
    "    print(\"Full Response Time (secs)\", time()-t0, flush=True)\n",
    "    if use_cache:\n",
    "        llmcache.store(user_query, response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d15051c",
   "metadata": {},
   "source": [
    "## Use the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "\n",
    "def response(query):\n",
    "    return generate_response(use_cache, llmcache, query, agent)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Start typing below and then click **Submit** to see the response.\")\n",
    "    with gr.Row():\n",
    "        inp = gr.Textbox(label=\"Prompt\", placeholder=\"Ask me anything about the BMW i4!\")\n",
    "        out = gr.Textbox(label=\"LLM Response\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton([inp, out])\n",
    "    btn.click(fn=response, inputs=inp, outputs=out)\n",
    "\n",
    "demo.launch(show_api=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
